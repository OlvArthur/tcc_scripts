{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit2816f1ca16c74fefba0c069c227312dd",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1,\"/home/arthur/TCC/codigo/twint\")\n",
    "sys.path.insert(1,\"twint/\")\n",
    "# print(sys.path)\n",
    "\n",
    "import twint\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import nltk \n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.Format = \"Username: {username} |  Tweet: {tweet}\"\n",
    "c.Limit = 500\n",
    "c.Pandas = True\n",
    "c.Username = \"mayarinnha\"\n",
    "c.Lang = \"pt\"\n",
    "c.Store_csv = True\n",
    "c.Output = \"../../datasets/twitter-questionnaire/\"+c.Username+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "não procurou\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test = pd.read_csv('/home/arthur/TCC/datasets/twitter-questionnaire/'+c.Username+'.csv', sep=',').filter(items=['username','tweet'])\n",
    "    print('não procurou')\n",
    "except FileNotFoundError:\n",
    "    twint.run.Search(c)\n",
    "    test = pd.read_csv('/home/arthur/TCC/datasets/twitter-questionnaire/'+c.Username+'.csv', sep=',').filter(items=['username','tweet'])  \n",
    "    print('procurou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy = test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern removal function creation\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy['tidy_tweets'] = np.vectorize(remove_pattern)(test_copy['tweet'],'@[\\w]*') \n",
    "test_copy.tidy_tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_url(data):\n",
    "    ls = []\n",
    "    words = ''\n",
    "    regexp1 = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    regexp2 = re.compile('www?.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    for line in data:\n",
    "        urls = regexp1.findall(line)\n",
    "\n",
    "        for u in urls:\n",
    "            line = line.replace(u, ' ')\n",
    "\n",
    "        urls = regexp2.findall(line)\n",
    "\n",
    "        for u in urls:\n",
    "            line = line.replace(u, ' ')\n",
    "            \n",
    "        ls.append(line)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy.tidy_tweets = _remove_url(test_copy.tidy_tweets)\n",
    "test_copy.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_txt):\n",
    "    input_txt = input_txt.replace('.',' ').replace('?',' ').replace('!',' ').replace(':', ' ').replace(',',' ').replace(';',' ').replace('(',' ').replace(')',' ').replace('-',' ').replace('#',' ')\n",
    "    return input_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy['tidy_tweets'] = test_copy['tidy_tweets'].apply(lambda x: remove_punctuation(x))\n",
    "test_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy.tidy_tweets = test_copy.tidy_tweets.str.lower()\n",
    "test_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_copy.tidy_tweets = test_copy.tidy_tweets.apply(lambda x: unicodedata.normalize('NFD',x).encode('ascii','ignore').decode('utf8') )\n",
    "# test_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = test_copy.tidy_tweets.apply(lambda x: x.split())\n",
    "tokenized_tweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_standardization(tokens, std_list):\n",
    "    ls = []\n",
    "\n",
    "    for tk_line in tokens:\n",
    "        new_tokens = []\n",
    "        for word in tk_line:\n",
    "            if word in std_list:\n",
    "                word = std_list[word]\n",
    "                \n",
    "            new_tokens.append(word) \n",
    "            \n",
    "        ls.append(new_tokens)\n",
    "\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list = {'eh': 'é', 'vc': 'você', 'vcs': 'vocês','tb': 'também', 'tbm': 'também', 'obg': 'obrigado', 'gnt': 'gente', 'q': 'que', 'n': 'não', 'cmg': 'comigo', 'p': 'para', 'ta': 'está', 'to': 'estou', 'vdd': 'verdade','pra': 'para', 'pro': 'para'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy['tidy_tweets'] = _apply_standardization(tokenized_tweet,std_list)\n",
    "test_copy.tidy_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_stopwords = stopwords.words('portuguese')\n",
    "stop_words = []\n",
    "noisy_words = ['.', '?', '!', ':', ',', ';', '(', ')', '-']\n",
    "\n",
    "stop_words.extend(pt_stopwords)\n",
    "stop_words.extend(noisy_words)\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy.tidy_tweets = test_copy.tidy_tweets.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_copy.tidy_tweets.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy.head(10)"
   ]
  }
 ]
}